# ğŸ“– Classification de textes avec NLP 

## ğŸ“Œ Description
Ce projet met en place un pipeline **NLP (Natural Language Processing)** afin de **classer des tweets selon leur genre ou leur ressenti (positif/nÃ©gatif)**.  

Le dataset utilisÃ© provient de **Kaggle / Tweets**, et contient des tweets.  

---

## ğŸ› ï¸ Technologies utilisÃ©es
- Python 3.x  
- Pandas (manipulation de donnÃ©es)  
- WordCloud (visualisation des mots frÃ©quents)  
- Scikit-learn (prÃ©traitement et modÃ¨les ML)  
- Matplotlib / Seaborn (visualisations)  

---

## ğŸ§  Approche technique

### 1. Exploration des donnÃ©es
- Analyse des longueurs de descriptions.  
- Comptage des classes (`Positive` / `Negative`).  
- Distribution des genres.  

### 2. PrÃ©traitement
- Nettoyage des textes : suppression de la ponctuation, stopwords, etc.  
- CrÃ©ation de la **variable cible** (`target`).  
- Visualisation des mots frÃ©quents par classe avec un **WordCloud**.  

### 3. Vectorisation
Pour transformer le texte brut en vecteurs exploitables par les modÃ¨les de machine learning, deux approches principales sont utilisÃ©es :  

#### ğŸ”¹ Bag of Words (BoW)
- Compte la frÃ©quence dâ€™apparition des mots dans un texte.  
- Exemple :  
  - Texte 1 : "le film est bon"  
  - Texte 2 : "le film est mauvais"  
  - Vocabulaire = {le, film, est, bon, mauvais}  
  - Vecteurs :  
    - T1 = [1,1,1,1,0]  
    - T2 = [1,1,1,0,1]  

Limite : tous les mots sont pondÃ©rÃ©s de la mÃªme faÃ§on â†’ les mots trÃ¨s frÃ©quents (ex. *le, est*) dominent.  

#### ğŸ”¹ TF-IDF (Term Frequency â€“ Inverse Document Frequency)
- AmÃ©liore BoW en rÃ©duisant lâ€™importance des mots frÃ©quents et en valorisant les mots plus discriminants.  
- Formule :  
  - **TF** = frÃ©quence dâ€™un mot dans un document  
  - **IDF** = log(N / df) oÃ¹ N = nombre de documents, df = nb de documents contenant le mot  
- Exemple : le mot *film* apparaÃ®t dans 100% des documents â†’ poids faible.  
- Le mot *mauvais* nâ€™apparaÃ®t que dans peu de documents â†’ poids Ã©levÃ©.  

ğŸ‘‰ En pratique, **TF-IDF donne de meilleurs rÃ©sultats** que BoW pour la classification de textes.  

### 4. ModÃ©lisation
- Mise en place de modÃ¨les de classification supervisÃ©e (ex. Logistic Regression, Naive Bayes, SVM).  
- EntraÃ®nement et Ã©valuation des performances.  

### 5. Ã‰valuation
- SÃ©paration train/test.  
- MÃ©triques :  
  - **Accuracy**  
  - **Precision**  
  - **Recall**  
  - **F1-score**  

---

## âš™ï¸ Installation
1. Cloner le dÃ©pÃ´t :
   ```bash
   git clone https://github.com/votre-utilisateur/nlp-text-classification.git
   cd nlp-text-classification
